{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MA-MOROB-M02: Trajectory Estimation (Semester 1)\n",
    "\n",
    "*part of the Programme \"Mobile Robotics\" (M.Sc.) at the Bonn University*\n",
    "\n",
    "## Methods for high precision trajectory estimation of mobile mapping vehicles on crop fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem Introduction\n",
    "\n",
    "<!--\n",
    "* Add robot data recording video here\n",
    "* explain to want to come back multiple times, monitor crop growth\n",
    "* introduce the problem of robot localisation\n",
    "* ask about other scenarios where precision localisation is needed\n",
    "* explain the simplicity of a row (1D) in the strawberry tunnel case\n",
    "-->\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls autoplay loop>\n",
    "      <source src=\"thorvald-data.mp4\" type=\"video/mp4\">\n",
    "</video></div>\n",
    "\n",
    "* An Example of Crop Monitoring with a Mobile Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Syllabus \"Trajectory Estimation\"\n",
    "\n",
    "1. Problem Introduction and introduction to the Module\n",
    "  * Learning Outcomes\n",
    "  * Assessment\n",
    "    * Oral Examination 50%\n",
    "    * Coursework (Jupyter Notebooks), 50%\n",
    "1. Representations of Trajectories\n",
    "  * 1 -- 3 D\n",
    "  * Sequence of States, usually in $SE(3)$\n",
    "  * we have to deal with *noise*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Global and Local frames of reference\n",
    "\n",
    "\n",
    "#### Local\n",
    "\n",
    "* \n",
    "* Dead reckoning, sources:\n",
    "  * encoders, IMU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classes of Probabilistic Sequential Models:\n",
    "\n",
    "*Probabilistic Graphical Models*\n",
    "\n",
    "<img src=\"pgm-models.svg\" alt=\"pgm\" width=\"50%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. **Current state estimation based on past states and observations**:\n",
    "   - **Filtering** algorithms such as:\n",
    "     - Kalman filter (for linear Gaussian systems)\n",
    "     - Extended Kalman filter (for nonlinear systems)\n",
    "     - Unscented Kalman filter (for highly nonlinear systems)\n",
    "     - Particle filter (for general non-Gaussian, nonlinear systems)\n",
    "   - These algorithms compute $P(x_t | y_{1:t})$ - the probability of the current state $x_t$ given all observations $y_{1:t}$ up to now ($t$)\n",
    "\n",
    "2. **Next state prediction**:\n",
    "   - **Prediction** algorithms including:\n",
    "     - Predictive step in Kalman filtering\n",
    "     - Forward models in dynamic Bayesian networks\n",
    "     - State transition models in hidden Markov models\n",
    "     - Forecast models in time series analysis\n",
    "   - These compute $P(x_{t+1} | y_{1:t})$ - predicting the next state given observations up to now\n",
    "\n",
    "3. **Most likely sequence of states from observation sequence**:\n",
    "   - **Smoothing** and **decoding** algorithms such as:\n",
    "     - Viterbi algorithm (for finding the most probable state sequence)\n",
    "     - Forward-backward algorithm (for computing marginal distributions)\n",
    "     - Rauch-Tung-Striebel smoother (for linear Gaussian systems)\n",
    "   - These compute \n",
    "        $$arg max_{x_{1:T}} P(x_{1:T} | y_{1:T})$$\n",
    "     i.e. the most likely complete state trajectory given all observations\n",
    "\n",
    "All of these tasks can be formulated within the framework of dynamic Bayesian networks, which are temporal extensions of probabilistic graphical models that explicitly represent the evolution of random variables over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prinicples of the Kalman Filter\n",
    "\n",
    "\n",
    "The Kalman filter manages to incorporate all past measurements implicitly through recursion.\n",
    "\n",
    "1. **State representation**: The Kalman filter maintains a sufficient statistic of all past information in the form of:\n",
    "   - A state estimate (mean vector)\n",
    "   - A covariance matrix (representing uncertainty)\n",
    "\n",
    "2. **Recursive nature**: The current state estimate already contains information from all previous measurements because:\n",
    "   - At each step t, the state estimate x̂(t|t) is based on x̂(t-1|t-1) and y(t)\n",
    "   - But x̂(t-1|t-1) was itself based on x̂(t-2|t-2) and y(t-1)\n",
    "   - And so on, all the way back to the initial state\n",
    "\n",
    "3. **Markov property**: The Kalman filter exploits the Markov property of the system - the current state contains all the information needed to predict future states, so we don't need to store the entire history.\n",
    "\n",
    "4. **Optimal property**: For linear Gaussian systems, this recursive estimation is provably optimal - the state estimate and covariance at time t are sufficient statistics for all measurements up to time t.\n",
    "\n",
    "So while the Kalman filter equations only explicitly reference the previous state estimate and current observation:\n",
    "\n",
    "```\n",
    "Predict:\n",
    "x̂(t|t-1) = F·x̂(t-1|t-1)\n",
    "P(t|t-1) = F·P(t-1|t-1)·F' + Q\n",
    "\n",
    "Update:\n",
    "K(t) = P(t|t-1)·H'·[H·P(t|t-1)·H' + R]^(-1)\n",
    "x̂(t|t) = x̂(t|t-1) + K(t)·[y(t) - H·x̂(t|t-1)]\n",
    "P(t|t) = [I - K(t)·H]·P(t|t-1)\n",
    "```\n",
    "\n",
    "It's actually integrating information from all past measurements through this recursive structure. This is one of the elegant properties of the Kalman filter - it provides a compact, computationally efficient way to incorporate the entire measurement history without having to store or reprocess that history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
